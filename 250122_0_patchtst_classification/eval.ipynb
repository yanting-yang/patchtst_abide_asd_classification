{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import nibabel as nib\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    text_model_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained text model name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    token: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n",
    "                \"generated when running `huggingface-cli login` (stored in `~/.huggingface`).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n",
    "                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n",
    "                \" code, as it will execute code present on the Hub on your local machine.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_vision_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the vision model parameters or not.\"}\n",
    "    )\n",
    "    freeze_text_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the text model parameters or not.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    data_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The data directory containing input files.\"})\n",
    "    image_column: Optional[str] = field(\n",
    "        default=\"image_path\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n",
    "    )\n",
    "    caption_column: Optional[str] = field(\n",
    "        default=\"caption\",\n",
    "        metadata={\n",
    "            \"help\": \"The name of the column in the datasets containing the image captions.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "dataset_path_mapping = {\n",
    "    \"./dataset_loading_scripts/abide.py\": (\"image_path\", \"caption\"),\n",
    "}\n",
    "\n",
    "\n",
    "def collate_fn(batch: list[dict]):\n",
    "    past_values = torch.stack([sample['time_series'] for sample in batch])\n",
    "    target_values = torch.stack([sample['label'] for sample in batch])\n",
    "    return {\n",
    "        'past_values': past_values,\n",
    "        'target_values': target_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    '--tokenizer_name', '../pretrained_models/roberta-base',\n",
    "    '--text_model_name', '../pretrained_models/roberta-base',\n",
    "    '--trust_remote_code',\n",
    "    '--freeze_text_model',\n",
    "    '--dataset_path', './dataset_loading_scripts/abide.py',\n",
    "    '--data_dir', '/bigdata/yanting/datasets/nilearn_data',\n",
    "    '--output_dir', './outputs',\n",
    "    '--overwrite_output_dir',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "    '--eval_strategy', 'epoch',\n",
    "    '--per_device_train_batch_size', '64',\n",
    "    '--per_device_eval_batch_size', '1',\n",
    "    '--learning_rate', '1e-4',\n",
    "    '--weight_decay', '1e-4',\n",
    "    '--num_train_epochs', '20',\n",
    "    '--lr_scheduler_type', 'cosine_with_restarts',\n",
    "    '--logging_steps', '1',\n",
    "    '--save_strategy', 'epoch',\n",
    "    '--save_safetensors', 'False',\n",
    "    '--dataloader_drop_last', 'True',\n",
    "    '--dataloader_num_workers', '8',\n",
    "    '--run_name', 'brainnettf_asd',\n",
    "    '--remove_unused_columns', 'False',\n",
    "    '--report_to', 'wandb'\n",
    "]\n",
    "\n",
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, DataTrainingArguments, TrainingArguments)\n",
    ")\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(\n",
    "    args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\n",
    "    path=data_args.dataset_path,\n",
    "    data_dir=data_args.data_dir,\n",
    "    split='train',\n",
    "    trust_remote_code=model_args.trust_remote_code\n",
    ").train_test_split(\n",
    "    test_size=.2,\n",
    "    stratify_by_column='label',\n",
    "    seed=42,\n",
    ")\n",
    "ds_train_val = ds['train']\n",
    "ds_test = ds['test']\n",
    "ds_train_val = ds_train_val.train_test_split(\n",
    "    test_size=.2,\n",
    "    stratify_by_column='label',\n",
    "    seed=42,\n",
    ")\n",
    "ds_train = ds_train_val['train']\n",
    "ds_val = ds_train_val['test']\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': ds_train,\n",
    "    'validation': ds_val,\n",
    "    'test': ds_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.PatchTSTConfig(\n",
    "        num_input_channels=200,\n",
    "        num_targets=2,\n",
    "        context_length=512,\n",
    "        patch_length=12,\n",
    "        stride=12,\n",
    "        use_cls_token=True,\n",
    "    )\n",
    "model = transformers.PatchTSTForClassification.from_pretrained(\n",
    "    last_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(batch):\n",
    "    time_series_lst = [np.loadtxt(\n",
    "        time_series_path, dtype=np.float32\n",
    "    ) for time_series_path in batch['time_series_path']]  # bs x sequence_length x num_input_channels\n",
    "\n",
    "    bs = len(time_series_lst)\n",
    "    sequence_length = 512\n",
    "    num_input_channels = time_series_lst[0].shape[-1]\n",
    "\n",
    "    mask = np.zeros(\n",
    "        (bs, sequence_length, num_input_channels), dtype=np.bool_\n",
    "    )\n",
    "\n",
    "    for i in range(len(time_series_lst)):\n",
    "        time_series = time_series_lst[i]\n",
    "        # truncate\n",
    "        if time_series.shape[0] > sequence_length:\n",
    "            time_series = time_series[:sequence_length]\n",
    "        # mask\n",
    "        mask[i, :time_series.shape[0]] = 1\n",
    "        # pad\n",
    "        time_series_lst[i] = np.pad(\n",
    "            time_series,\n",
    "            ((0, sequence_length - time_series.shape[0]), (0, 0))\n",
    "        )\n",
    "    time_series_lst = np.stack(time_series_lst, axis=0)\n",
    "\n",
    "    batch['time_series'] = torch.from_numpy(time_series_lst)\n",
    "    batch['mask'] = torch.from_numpy(mask)\n",
    "    batch['label'] = torch.tensor(batch['label'])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset[\"test\"]\n",
    "if data_args.max_eval_samples is not None:\n",
    "    max_eval_samples = min(\n",
    "        len(eval_dataset), data_args.max_eval_samples)\n",
    "    eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "\n",
    "# Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "eval_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatchTSTForClassification(\n",
       "  (model): PatchTSTModel(\n",
       "    (scaler): PatchTSTScaler(\n",
       "      (scaler): PatchTSTStdScaler()\n",
       "    )\n",
       "    (patchifier): PatchTSTPatchify()\n",
       "    (masking): Identity()\n",
       "    (encoder): PatchTSTEncoder(\n",
       "      (embedder): PatchTSTEmbedding(\n",
       "        (input_embedding): Linear(in_features=36, out_features=512, bias=True)\n",
       "      )\n",
       "      (positional_encoder): PatchTSTPositionalEncoding(\n",
       "        (positional_dropout): Identity()\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): PatchTSTEncoderLayer(\n",
       "          (self_attn): PatchTSTAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_path1): Identity()\n",
       "          (norm_sublayer1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): GELUActivation()\n",
       "            (2): Identity()\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_path3): Identity()\n",
       "          (norm_sublayer3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): PatchTSTClassificationHead(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dropout): Identity()\n",
       "    (linear): Linear(in_features=102400, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:06<00:00, 27.19it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "y_score = []\n",
    "for sample in tqdm(eval_dataset):\n",
    "    labels = sample['label']\n",
    "    mask = sample['mask']\n",
    "    pixel_values = sample['time_series']\n",
    "    y_true.append(labels)\n",
    "    with torch.no_grad():\n",
    "        res = model(pixel_values.unsqueeze(0).to(device))\n",
    "        logits = res['prediction_logits']\n",
    "        y_score.append(logits.squeeze(0).cpu().numpy())\n",
    "        y_pred.append(logits.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1]), array([94, 81])), (array([0, 1]), array([168,   7])))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_true, return_counts=True), np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.62306277909115, 54.285714285714285, 4.938271604938271, 96.80851063829788)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc = float(sklearn.metrics.roc_auc_score(y_true, np.array(y_score)[:,1]))\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "sen = sklearn.metrics.recall_score(y_true, y_pred)\n",
    "spc = sklearn.metrics.recall_score(y_true, y_pred, pos_label=0)\n",
    "auroc*100, acc*100, sen*100, spc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
